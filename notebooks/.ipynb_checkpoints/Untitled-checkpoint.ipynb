{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab87280-39f3-4b9b-b680-1cfc2c4ecc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b7ec86-e276-4431-8524-e983646bb1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(DEVICE)\n",
    "\n",
    "PIN_MEMORY = True if DEVICE == \"cuda\" else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f51a0f0-b911-45eb-b492-d3a9c7b7845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"\"\n",
    "MASK_DATASET_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa45564d-739a-45d6-8405-80c2ee238241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set all the hyper parameters\n",
    "\n",
    "NUM_CHANNELS = 1\n",
    "NUM_CLASSES = 1\n",
    "NUM_LEVELS = 3\n",
    "\n",
    "LR = 0.0001\n",
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "INPUT_IMAGE_HEIGHT = 512\n",
    "INPUT_IMAGE_WIDTH = 512\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "BASE_OUTPUT = \"output\"\n",
    "\n",
    "MODEL_PATH = \"\"\n",
    "PLOT_PATH = \"\"\n",
    "TEST_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017396cb-18c4-4f2e-b0c7-739c43a3c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the custom segmentation dataset class\n",
    "from torch.utils.data import Dataset\n",
    "def cv2():\n",
    "    def imread(i):\n",
    "        pass\n",
    "\n",
    "    def cvtColor(i, s):\n",
    "        pass\n",
    "\n",
    "    def imread(i, s):\n",
    "        pass\n",
    "    \n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, imagePaths, maskPaths, transforms):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.imagePaths = imagePaths\n",
    "        self.maskPaths = imagePaths\n",
    "        self.transforms = transforms\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.imagePath)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            imagePath = self.imagePaths[idx]\n",
    "\n",
    "            image = cv2.imread(imagePath)\n",
    "            image = cv2.cvtColor(image,  cv2.COLOR_BGR2RGB)\n",
    "            mask = cv2.imread(self.maskPaths[idx], 0)\n",
    "\n",
    "            if self.transforms is not None:\n",
    "                image = self.transforms(image)\n",
    "                mask = self.transforms(mask)\n",
    "\n",
    "            return image, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "549935c4-393c-434b-a3e2-389472fff44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class litsUnet(nn.Module):\n",
    "    def __init__(self, n_class: tuple):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        # input: 572X572X3\n",
    "        self.encoder11 = nn.Conv2d(3, 64, kernel_size= 3, padding= 1)\n",
    "        self.encoder12 = nn.Conv2d(64, 64, kernel_size= 3, padding= 1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size= 2, stride= 2)\n",
    "\n",
    "        self.encoder21 = nn.Conv2d(64, 128, kernel_size= 3, padding= 1)\n",
    "        self.encoder22 = nn.Conv2d(128, 128, kernel_size= 2, padding= 1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size= 2, stride= 2)\n",
    "\n",
    "        self.encoder31 = nn.Conv2d(128, 256, kernel_size= 3, padding= 1)\n",
    "        self.encoder32 = nn.Conv2d(256, 256, kernel_size= 3, padding= 1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size= 3, stride= 2)\n",
    "\n",
    "        self.encoder41 = nn.Conv2d(256, 512, kernel_size= 3, padding= 1)\n",
    "        self.encoder42 = nn.Conv2d(512, 512, kernel_size= 3, padding= 1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size= 2, stride= 2)\n",
    "\n",
    "        self.encoder51 = nn.Conv2d(512, 1024, kernel_size= 3, padding= 1)\n",
    "        self.encoder52 = nn.Conv2d(1024, 1024, kernel_size= 3, padding= 1)\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size= 2, stride= 2)\n",
    "        self.decoder11 = nn.Conv2d(1024, 512, kernel_size= 3, padding= 1)\n",
    "        self.decoder12 = nn.Conv2d(512, 512, kernel_size= 3, padding= 1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size= 2, stride= 2)\n",
    "        self.decoder21 = nn.Conv2d(512, 256, kernel_size= 3, padding= 1)\n",
    "        self.decoder22 = nn.Conv2d(256, 256, kernel_size= 3, padding= 1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size= 2, stride= 2)\n",
    "        self.decoder31 = nn.Conv2d(256, 128, kernel_size= 2, padding= 1)\n",
    "        self.decode32 = nn.Conv2d(128, 128, kernel_size= 2, padding= 1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size= 2, stride= 2)\n",
    "        self.decoder41 = nn.Conv2d(128, 64, kernel_size= 2, padding= 1)\n",
    "        self.decoder42 = nn.Conv2d(64, 64, kernel_size= 2, padding= 1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size= 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x_encoder11 = nn.ReLU(self.encoder11(x))\n",
    "        x_encoder12 = nn.ReLU(self.encoder12(x_encoder11))\n",
    "        x_pool1 = self.pool1(xencoder12)\n",
    "\n",
    "        x_encoder21 = nn.ReLU(self.encoder_21(x_pool1))\n",
    "        x_encoder22 = nn.ReLU(self.encoder_22(x_encoder21))\n",
    "        x_pool2 = self.pool2(x_encoder22)\n",
    "\n",
    "        x_encoder31 = nn.ReLu(self.encoder31(x_pool2))\n",
    "        x_encoder32 = nn.ReLU(self.encoder32(x_encoder31))\n",
    "        x_pool3 = self.pool3(x_encoder32)\n",
    "\n",
    "        x_encoder41 = nn.ReLU(self.encoder41(x_pool3))\n",
    "        x_encoder42 = nn.ReLU(self.encoder42(x_encoder41))\n",
    "        x_pool4 = self.pool4(x_encoder42)\n",
    "\n",
    "        x_encoder51 = nn.ReLU(self.encoder51(x_pool4))\n",
    "        x_encoder52 = nn.ReLU(self.encoder52(x_encoder51))\n",
    "\n",
    "        # Decoder\n",
    "\n",
    "        xu1 = self.upconv(x_encoder52)\n",
    "        xu11 = torch.cat([xu1, x_encoder42], dim= 1)\n",
    "        x_decoder11 = nn.ReLU(self.decoder11(xu11))\n",
    "        x_decoder12 = nn.ReLU(self.decoder12(x_decoder11))\n",
    "\n",
    "        xu2 = self.upconv2(x_decoder12)\n",
    "        xu22 = torch.cat([xu2, x_encoder32], dim= 1)\n",
    "        x_decoder21 = nn.ReLU(self.decoder21(xu22))\n",
    "        x_decoder22 = nn.ReLU(self.decoder22(x_decoder21))\n",
    "\n",
    "        xu3 = self.upconv2(x_decoder22)\n",
    "        xu33 = torch.cat([xu3, x_encoder22], dim= 1)\n",
    "        x_decoder31 = nn.ReLU(self.decoder31(xu33))\n",
    "        x_decoder32 = nn.ReLU(self.decoder32(x_decoder31))\n",
    "\n",
    "        xu4 = self.upconv4(x_decoder32)\n",
    "        xu44 = torch.cat([xu4, x_encoder12], dim = 1)\n",
    "        x_decoder41 = nn.ReLU(self.decoder41(xu44))\n",
    "        x_decoder42 = nn.ReLU(self.decoder42(x_decoder41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(x_decoder42)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fa908bf-9b81-4725-a5a8-39672ca2ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = \"\"\n",
    "train_masks= \"\"\n",
    "test_images = \"\"\n",
    "test_masks = \"\"\n",
    "\n",
    "transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                transforms.Resize((INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH)),\n",
    "                                transforms.ToTensor()\n",
    "                                ])\n",
    "\n",
    "train_dataset = SegmentationDataset(imagePaths = train_images, maskPaths = train_masks, transforms = transforms)\n",
    "\n",
    "test_dataset = SegmentationDataset(imagePaths = test_images, maskPaths = test_masks, transforms = transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a69d4f-6b2b-4bd0-974d-c3b3cbc8fc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
